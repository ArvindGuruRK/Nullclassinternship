# Emotion_detection

### **Emotion Detection Using Machine Learning**

#### **Introduction:**
Emotion detection plays a crucial role in various real-world applications, from enhancing user experiences to improving human-computer interactions. This project focuses on developing a machine learning-based solution to identify human emotions through text or other data modalities such as audio or visual inputs.

#### **Project Overview:**
The primary goal of this project was to create a system that can classify emotions using machine learning algorithms. Key steps in the project included data collection, preprocessing, feature extraction, and model training. Various machine learning algorithms were implemented and evaluated to identify emotions such as happiness, sadness, anger, surprise, and more.

#### **What I Created:**
Through this project, I built an emotion detection model capable of accurately analyzing input data and classifying emotions. The project involved:
- **Data Processing:** Cleaning and organizing the dataset to ensure optimal model performance.
- **Feature Engineering:** Extracting relevant features that effectively represent emotional patterns.
- **Algorithm Implementation:** Implementing and comparing various machine learning algorithms such as Support Vector Machines (SVM), Decision Trees, Random Forest, and Logistic Regression.
- **Model Evaluation:** Using performance metrics like accuracy, precision, and recall to evaluate the models and select the best-performing algorithm.

#### **Outcomes:**
- Successfully trained a machine learning model capable of detecting emotions with high accuracy.
- Demonstrated the effectiveness of different algorithms in emotion recognition tasks.
- Gained hands-on experience in data preprocessing, feature extraction, and the practical application of machine learning models.
- Enhanced my understanding of emotion recognition and its potential real-world applications in fields such as customer service, healthcare, and sentiment analysis.

# TASK 1
# Visualize Activation Maps

For my internship tasks related to the Emotion Detection project, Task 1 focused on visualizing activation maps to gain insights into the image regions that activate specific filters in the pre-trained model for emotion detection. This task aimed to enhance interpretability by identifying the features and patterns the model relies on for classifying emotions. Using the pre-trained model, I generated activation maps to analyze the areas of interest, such as facial landmarks, expressions, and subtle texture variations that contribute to emotion recognition. Key features extracted included the position and shape of the eyes, mouth, and eyebrows, as well as overall facial structure and symmetry. This task provided a deeper understanding of the model's decision-making process and helped to validate the effectiveness of the extracted features in detecting emotions. A GUI was not required for this task, allowing a streamlined focus on model visualization and analysis.


![Screenshot 2024-12-27 112557](https://github.com/user-attachments/assets/54d3b4b7-3452-4496-8707-4ab7ef3e737c)



#### **CNN ACTIVATION MAPS THAT EXTRACTS THE HIGHLY FEATURED IMAGE:**


![Screenshot 2024-12-27 112229](https://github.com/user-attachments/assets/bd41a719-edea-44e3-bfcb-823948e1b3b3)



# TASK 2
# Attendance System Model

For my internship tasks related to the **Emotion Detection** project, Task 1 focused on visualizing activation maps to gain insights into the image regions that activate specific filters in the pre-trained model for emotion detection. This task aimed to enhance interpretability by identifying the features and patterns the model relies on for classifying emotions. Using the pre-trained model, I generated activation maps to analyze the areas of interest, such as facial landmarks, expressions, and subtle texture variations that contribute to emotion recognition. Key features extracted included the position and shape of the eyes, mouth, and eyebrows, as well as overall facial structure and symmetry. This task provided a deeper understanding of the model's decision-making process and helped to validate the effectiveness of the extracted features in detecting emotions. A GUI was not required for this task, allowing a streamlined focus on model visualization and analysis. 


![Screenshot 2024-12-27 100054](https://github.com/user-attachments/assets/e7eb30e3-31d9-45b3-b110-0f2da7327e44)

![Screenshot 2024-12-27 094550](https://github.com/user-attachments/assets/fa205f6a-28f4-4b7f-a522-767dae1a0d51)

![Screenshot 2024-12-27 094527](https://github.com/user-attachments/assets/c11e797d-d5b9-4dc9-9675-6cb3fac038f9)

![Screenshot 2024-12-27 094450](https://github.com/user-attachments/assets/0c2449d3-e4d8-49ea-960d-cae939da22f4)

![Screenshot 2024-12-27 094052](https://github.com/user-attachments/assets/e313910b-bf52-4677-abeb-dd4a57bca691)

![Screenshot 2024-12-27 093904](https://github.com/user-attachments/assets/9e2a6d8b-d532-4304-9d7f-a3ff256a2839)
